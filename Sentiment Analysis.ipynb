{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>favs</th>\n",
       "      <th>retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@NateSilver538 Tesla makes cars with sophistic...</td>\n",
       "      <td>Thu Mar 19 04:16:37 +0000 2020</td>\n",
       "      <td>10478</td>\n",
       "      <td>1890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tesla's Musk offers to make ventilators amid s...</td>\n",
       "      <td>Thu Mar 19 08:00:33 +0000 2020</td>\n",
       "      <td>1039</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Elon Musk says Tesla is willing to make ventil...</td>\n",
       "      <td>Thu Mar 19 15:15:01 +0000 2020</td>\n",
       "      <td>1015</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @SamanthaTheEA: @elonmusk there is not one ...</td>\n",
       "      <td>Thu Mar 19 19:52:25 +0000 2020</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Jennerator211 @Tesla You don‚Äôt have to now......</td>\n",
       "      <td>Thu Mar 19 19:52:24 +0000 2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @NateSilver538 Tesla makes cars with sophistic...   \n",
       "1  Tesla's Musk offers to make ventilators amid s...   \n",
       "2  Elon Musk says Tesla is willing to make ventil...   \n",
       "3  RT @SamanthaTheEA: @elonmusk there is not one ...   \n",
       "4  @Jennerator211 @Tesla You don‚Äôt have to now......   \n",
       "\n",
       "                             date   favs  retweet  \n",
       "0  Thu Mar 19 04:16:37 +0000 2020  10478     1890  \n",
       "1  Thu Mar 19 08:00:33 +0000 2020   1039      302  \n",
       "2  Thu Mar 19 15:15:01 +0000 2020   1015      238  \n",
       "3  Thu Mar 19 19:52:25 +0000 2020      0       11  \n",
       "4  Thu Mar 19 19:52:24 +0000 2020      0        0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tesla = pd.read_csv(\"Tesla.csv\")\n",
    "tesla.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>favs</th>\n",
       "      <th>retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WHY IS DOLLAR DYING?  2008 Fed printed $4.5 Tr...</td>\n",
       "      <td>Tue Apr 07 12:14:18 +0000 2020</td>\n",
       "      <td>6867</td>\n",
       "      <td>2030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The argument of \"the Bitcoin halving is priced...</td>\n",
       "      <td>Mon Apr 06 12:51:25 +0000 2020</td>\n",
       "      <td>1641</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Billionaire and former Facebook exec makes the...</td>\n",
       "      <td>Mon Apr 06 14:27:38 +0000 2020</td>\n",
       "      <td>771</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @SamTheCarpetMan: @Breaking911 Good, perhap...</td>\n",
       "      <td>Wed Apr 08 00:32:16 +0000 2020</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@LordoftheTrends @DrBitcoinMD What exactly in ...</td>\n",
       "      <td>Wed Apr 08 00:32:15 +0000 2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  WHY IS DOLLAR DYING?  2008 Fed printed $4.5 Tr...   \n",
       "1  The argument of \"the Bitcoin halving is priced...   \n",
       "2  Billionaire and former Facebook exec makes the...   \n",
       "3  RT @SamTheCarpetMan: @Breaking911 Good, perhap...   \n",
       "4  @LordoftheTrends @DrBitcoinMD What exactly in ...   \n",
       "\n",
       "                             date  favs  retweet  \n",
       "0  Tue Apr 07 12:14:18 +0000 2020  6867     2030  \n",
       "1  Mon Apr 06 12:51:25 +0000 2020  1641      285  \n",
       "2  Mon Apr 06 14:27:38 +0000 2020   771      249  \n",
       "3  Wed Apr 08 00:32:16 +0000 2020     0        1  \n",
       "4  Wed Apr 08 00:32:15 +0000 2020     0        0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bitcoin = pd.read_csv(\"full bitcoin.csv\")\n",
    "bitcoin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>favs</th>\n",
       "      <th>retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WHY IS DOLLAR DYING?  2008 Fed printed $4.5 Tr...</td>\n",
       "      <td>Tue Apr 07 12:14:18 +0000 2020</td>\n",
       "      <td>8179</td>\n",
       "      <td>2399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The same people who think bitcoin is 'dead' at...</td>\n",
       "      <td>Tue Apr 07 12:47:24 +0000 2020</td>\n",
       "      <td>2124</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You can now watch my full interview with @theR...</td>\n",
       "      <td>Tue Apr 07 11:56:46 +0000 2020</td>\n",
       "      <td>409</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @krakenfx: *NEW* KRAKEN RELEASES MARCH #BIT...</td>\n",
       "      <td>Wed Apr 08 17:00:41 +0000 2020</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @MeltedCrypto: 1,000 $XRP to 1 random #Foll...</td>\n",
       "      <td>Wed Apr 08 17:00:40 +0000 2020</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  WHY IS DOLLAR DYING?  2008 Fed printed $4.5 Tr...   \n",
       "1  The same people who think bitcoin is 'dead' at...   \n",
       "2  You can now watch my full interview with @theR...   \n",
       "3  RT @krakenfx: *NEW* KRAKEN RELEASES MARCH #BIT...   \n",
       "4  RT @MeltedCrypto: 1,000 $XRP to 1 random #Foll...   \n",
       "\n",
       "                             date  favs  retweet  \n",
       "0  Tue Apr 07 12:14:18 +0000 2020  8179     2399  \n",
       "1  Tue Apr 07 12:47:24 +0000 2020  2124      177  \n",
       "2  Tue Apr 07 11:56:46 +0000 2020   409       87  \n",
       "3  Wed Apr 08 17:00:41 +0000 2020     0        1  \n",
       "4  Wed Apr 08 17:00:40 +0000 2020     0       22  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bitcoin2 = pd.read_csv(\"full bitcoin 2.csv\")\n",
    "bitcoin2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>favs</th>\n",
       "      <th>retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>regarding the Valorant anticheat always runnin...</td>\n",
       "      <td>Fri Apr 17 16:35:15 +0000 2020</td>\n",
       "      <td>4784</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There‚Äôs a fake interview going around on Faceb...</td>\n",
       "      <td>Sat Apr 18 09:46:37 +0000 2020</td>\n",
       "      <td>1484</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Weekend Trap? Bitcoin Price Hits $7.3K in Atte...</td>\n",
       "      <td>Sat Apr 18 17:59:21 +0000 2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @biconnections: üö®Fast #Bitcoin #Crypto #Giv...</td>\n",
       "      <td>Sat Apr 18 17:59:20 +0000 2020</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @Paolo62103142: You have your own #website ...</td>\n",
       "      <td>Sat Apr 18 17:59:20 +0000 2020</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  regarding the Valorant anticheat always runnin...   \n",
       "1  There‚Äôs a fake interview going around on Faceb...   \n",
       "2  Weekend Trap? Bitcoin Price Hits $7.3K in Atte...   \n",
       "3  RT @biconnections: üö®Fast #Bitcoin #Crypto #Giv...   \n",
       "4  RT @Paolo62103142: You have your own #website ...   \n",
       "\n",
       "                             date  favs  retweet  \n",
       "0  Fri Apr 17 16:35:15 +0000 2020  4784      346  \n",
       "1  Sat Apr 18 09:46:37 +0000 2020  1484      108  \n",
       "2  Sat Apr 18 17:59:21 +0000 2020     0        0  \n",
       "3  Sat Apr 18 17:59:20 +0000 2020     0       38  \n",
       "4  Sat Apr 18 17:59:20 +0000 2020     0        1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bitcoin3 = pd.read_csv(\"full bitcoin 3.csv\")\n",
    "bitcoin3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VADER Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3575"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = bitcoin.append(bitcoin2, ignore_index = True)\n",
    "df.drop_duplicates(subset = \"text\", inplace = True)\n",
    "\n",
    "df = df.append(bitcoin3, ignore_index = True)\n",
    "df.drop_duplicates(subset = \"text\", inplace = True)\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vaderSentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-833497b0d90b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvaderSentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvaderSentiment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0manalyser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Defining function to extract polarity scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vaderSentiment'"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Defining function to extract polarity scores\n",
    "def sentiment_analyzer_scores(sentence):\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    return score[\"compound\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>favs</th>\n",
       "      <th>retweet</th>\n",
       "      <th>vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WHY IS DOLLAR DYING?  2008 Fed printed $4.5 Tr...</td>\n",
       "      <td>Tue Apr 07 12:14:18 +0000 2020</td>\n",
       "      <td>6867</td>\n",
       "      <td>2030</td>\n",
       "      <td>0.8434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The argument of \"the Bitcoin halving is priced...</td>\n",
       "      <td>Mon Apr 06 12:51:25 +0000 2020</td>\n",
       "      <td>1641</td>\n",
       "      <td>285</td>\n",
       "      <td>-0.4588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Billionaire and former Facebook exec makes the...</td>\n",
       "      <td>Mon Apr 06 14:27:38 +0000 2020</td>\n",
       "      <td>771</td>\n",
       "      <td>249</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @SamTheCarpetMan: @Breaking911 Good, perhap...</td>\n",
       "      <td>Wed Apr 08 00:32:16 +0000 2020</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@LordoftheTrends @DrBitcoinMD What exactly in ...</td>\n",
       "      <td>Wed Apr 08 00:32:15 +0000 2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RT @EtherSmart_ORG: #Ethersmart keep growing s...</td>\n",
       "      <td>Wed Apr 08 00:32:12 +0000 2020</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>0.8713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RT @BTC_Prometheus: #Giveaway $60 BTC\\n\\nOne p...</td>\n",
       "      <td>Wed Apr 08 00:32:11 +0000 2020</td>\n",
       "      <td>0</td>\n",
       "      <td>693</td>\n",
       "      <td>0.6908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RT @BitSler: Hello Guys! How are you! üòé\\n\\n10 ...</td>\n",
       "      <td>Wed Apr 08 00:32:09 +0000 2020</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0.7716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RT @Dehkunle: What‚Äôs stopping you from investi...</td>\n",
       "      <td>Wed Apr 08 00:32:02 +0000 2020</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>-0.1531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RT @YourCryptoJedi: Use Lolli üç≠to earn bitcoin...</td>\n",
       "      <td>Wed Apr 08 00:32:00 +0000 2020</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  WHY IS DOLLAR DYING?  2008 Fed printed $4.5 Tr...   \n",
       "1  The argument of \"the Bitcoin halving is priced...   \n",
       "2  Billionaire and former Facebook exec makes the...   \n",
       "3  RT @SamTheCarpetMan: @Breaking911 Good, perhap...   \n",
       "4  @LordoftheTrends @DrBitcoinMD What exactly in ...   \n",
       "5  RT @EtherSmart_ORG: #Ethersmart keep growing s...   \n",
       "6  RT @BTC_Prometheus: #Giveaway $60 BTC\\n\\nOne p...   \n",
       "7  RT @BitSler: Hello Guys! How are you! üòé\\n\\n10 ...   \n",
       "8  RT @Dehkunle: What‚Äôs stopping you from investi...   \n",
       "9  RT @YourCryptoJedi: Use Lolli üç≠to earn bitcoin...   \n",
       "\n",
       "                             date  favs  retweet   vader  \n",
       "0  Tue Apr 07 12:14:18 +0000 2020  6867     2030  0.8434  \n",
       "1  Mon Apr 06 12:51:25 +0000 2020  1641      285 -0.4588  \n",
       "2  Mon Apr 06 14:27:38 +0000 2020   771      249  0.0000  \n",
       "3  Wed Apr 08 00:32:16 +0000 2020     0        1  0.8162  \n",
       "4  Wed Apr 08 00:32:15 +0000 2020     0        0  0.0000  \n",
       "5  Wed Apr 08 00:32:12 +0000 2020     0      125  0.8713  \n",
       "6  Wed Apr 08 00:32:11 +0000 2020     0      693  0.6908  \n",
       "7  Wed Apr 08 00:32:09 +0000 2020     0       41  0.7716  \n",
       "8  Wed Apr 08 00:32:02 +0000 2020     0      104 -0.1531  \n",
       "9  Wed Apr 08 00:32:00 +0000 2020     0        1  0.6588  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"vader\"] = df[\"text\"].apply(sentiment_analyzer_scores)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>fullname</th>\n",
       "      <th>url</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>replies</th>\n",
       "      <th>likes</th>\n",
       "      <th>retweets</th>\n",
       "      <th>text</th>\n",
       "      <th>vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.078873e+18</td>\n",
       "      <td>TheBitfi</td>\n",
       "      <td>Bitfi - open source: bitfi.dev</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-12-29</td>\n",
       "      <td>9.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>The first lab made diamond happened in  amp no...</td>\n",
       "      <td>0.7678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.862542e+17</td>\n",
       "      <td>keno_sanders</td>\n",
       "      <td>Keno Sanders</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-04-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Great talk and discussions about Bitcoin vs Eu...</td>\n",
       "      <td>0.7650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.616666e+17</td>\n",
       "      <td>CryptoGamer_</td>\n",
       "      <td>CryptoGamer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-02-08</td>\n",
       "      <td>23.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>When things are down remember its never as bad...</td>\n",
       "      <td>0.0387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.354113e+17</td>\n",
       "      <td>real_vijay</td>\n",
       "      <td>Vijay Boyapati</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>12.0</td>\n",
       "      <td>509.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>While there are no a priori rules about the p...</td>\n",
       "      <td>0.4588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.005368e+18</td>\n",
       "      <td>Ok_coinexchange</td>\n",
       "      <td>OK.NET</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-06-09</td>\n",
       "      <td>39.0</td>\n",
       "      <td>324.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>Wow We reached K followers just within  weeks ...</td>\n",
       "      <td>0.9217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.250705e+17</td>\n",
       "      <td>Annrhefn</td>\n",
       "      <td>Annrhefn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-01-27</td>\n",
       "      <td>19.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>Central Banking created Debt Serfdom This is a...</td>\n",
       "      <td>-0.1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9.644832e+17</td>\n",
       "      <td>MScDigital</td>\n",
       "      <td>Digital Currency</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-02-16</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1598.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>Professional Certifications that will help jum...</td>\n",
       "      <td>0.4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9.083219e+17</td>\n",
       "      <td>pierre_rochard</td>\n",
       "      <td>Pierre Rochard [??????]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-09-14</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1248.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>I would not be interested in Bitcoin if govern...</td>\n",
       "      <td>0.1139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.026876e+18</td>\n",
       "      <td>ShaniNiazi786</td>\n",
       "      <td>Zeeshan Asmat Khan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-07</td>\n",
       "      <td>85.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>I will choose  Random Followers to Win  FOLLOW...</td>\n",
       "      <td>0.7925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.078984e+18</td>\n",
       "      <td>Plasma_Pay</td>\n",
       "      <td>PlasmaPay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-12-29</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>Remiit a blockchainpowered remittance platform...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.076899e+18</td>\n",
       "      <td>StandUpAndRIOT</td>\n",
       "      <td>CryptoFelix ??</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-12-23</td>\n",
       "      <td>18.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>The bear market came and it broke us Not in th...</td>\n",
       "      <td>-0.8074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.033343e+18</td>\n",
       "      <td>toqqnproject</td>\n",
       "      <td>Toqqn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>True story Sad and unfortunate that even great...</td>\n",
       "      <td>0.2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9.785573e+17</td>\n",
       "      <td>SharesChain</td>\n",
       "      <td>SharesChain Blockchain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-03-27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>Currently the transference of ownership or pur...</td>\n",
       "      <td>0.1779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>9.358888e+17</td>\n",
       "      <td>hkanji</td>\n",
       "      <td>hussein kanji</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-11-29</td>\n",
       "      <td>5.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>How many days it took Bitcoin to get from    d...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9.420835e+17</td>\n",
       "      <td>brucefenton</td>\n",
       "      <td>Bruce Fenton</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-12-16</td>\n",
       "      <td>112.0</td>\n",
       "      <td>4106.0</td>\n",
       "      <td>1353.0</td>\n",
       "      <td>HODLBuckle inStop gawking at pricesGo to confe...</td>\n",
       "      <td>0.6908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9.923380e+17</td>\n",
       "      <td>jakalonn</td>\n",
       "      <td>Jakalon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-05-04</td>\n",
       "      <td>2.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Uzun zamandr ertelediim yeni yatrmc iin kripto...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.062137e+18</td>\n",
       "      <td>GetDeepOnion</td>\n",
       "      <td>DeepOnion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-11-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>DeepVault is a blockchain notary that allows y...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.069324e+18</td>\n",
       "      <td>pwr_coin</td>\n",
       "      <td>PWR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-12-02</td>\n",
       "      <td>16.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>We are almost there MercaTox here we come PWRC...</td>\n",
       "      <td>0.0772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.077287e+18</td>\n",
       "      <td>elyqdcom</td>\n",
       "      <td>eLYQD.world</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-12-24</td>\n",
       "      <td>2.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>A silent night a star above a blessed gift of ...</td>\n",
       "      <td>0.9274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.078123e+18</td>\n",
       "      <td>AdminXET</td>\n",
       "      <td>XETOfficialAdmin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>2.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>News Update Another pleasure to announce that...</td>\n",
       "      <td>0.8672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id             user                        fullname  url  \\\n",
       "0   1.078873e+18         TheBitfi  Bitfi - open source: bitfi.dev  NaN   \n",
       "1   9.862542e+17     keno_sanders                    Keno Sanders  NaN   \n",
       "2   9.616666e+17     CryptoGamer_                     CryptoGamer  NaN   \n",
       "3   9.354113e+17       real_vijay                  Vijay Boyapati  NaN   \n",
       "4   1.005368e+18  Ok_coinexchange                          OK.NET  NaN   \n",
       "5   8.250705e+17         Annrhefn                        Annrhefn  NaN   \n",
       "6   9.644832e+17       MScDigital                Digital Currency  NaN   \n",
       "7   9.083219e+17   pierre_rochard         Pierre Rochard [??????]  NaN   \n",
       "8   1.026876e+18    ShaniNiazi786              Zeeshan Asmat Khan  NaN   \n",
       "9   1.078984e+18       Plasma_Pay                       PlasmaPay  NaN   \n",
       "10  1.076899e+18   StandUpAndRIOT                  CryptoFelix ??  NaN   \n",
       "11  1.033343e+18     toqqnproject                           Toqqn  NaN   \n",
       "12  9.785573e+17      SharesChain          SharesChain Blockchain  NaN   \n",
       "13  9.358888e+17           hkanji                   hussein kanji  NaN   \n",
       "14  9.420835e+17      brucefenton                    Bruce Fenton  NaN   \n",
       "15  9.923380e+17         jakalonn                         Jakalon  NaN   \n",
       "16  1.062137e+18     GetDeepOnion                       DeepOnion  NaN   \n",
       "17  1.069324e+18         pwr_coin                             PWR  NaN   \n",
       "18  1.077287e+18         elyqdcom                     eLYQD.world  NaN   \n",
       "19  1.078123e+18         AdminXET                XETOfficialAdmin  NaN   \n",
       "\n",
       "     timestamp  replies   likes  retweets  \\\n",
       "0   2018-12-29      9.0    61.0      16.0   \n",
       "1   2018-04-17      1.0     7.0       1.0   \n",
       "2   2018-02-08     23.0   135.0      51.0   \n",
       "3   2017-11-28     12.0   509.0     238.0   \n",
       "4   2018-06-09     39.0   324.0     160.0   \n",
       "5   2017-01-27     19.0   267.0     155.0   \n",
       "6   2018-02-16     90.0  1598.0     502.0   \n",
       "7   2017-09-14     19.0  1248.0     434.0   \n",
       "8   2018-08-07     85.0    95.0      87.0   \n",
       "9   2018-12-29      2.0   100.0     132.0   \n",
       "10  2018-12-23     18.0   146.0      29.0   \n",
       "11  2018-08-25      1.0    30.0      16.0   \n",
       "12  2018-03-27      0.0    74.0      78.0   \n",
       "13  2017-11-29      5.0   135.0     110.0   \n",
       "14  2017-12-16    112.0  4106.0    1353.0   \n",
       "15  2018-05-04      2.0   106.0      44.0   \n",
       "16  2018-11-12      0.0    34.0      21.0   \n",
       "17  2018-12-02     16.0    54.0      60.0   \n",
       "18  2018-12-24      2.0    92.0     119.0   \n",
       "19  2018-12-27      2.0    45.0     102.0   \n",
       "\n",
       "                                                 text   vader  \n",
       "0   The first lab made diamond happened in  amp no...  0.7678  \n",
       "1   Great talk and discussions about Bitcoin vs Eu...  0.7650  \n",
       "2   When things are down remember its never as bad...  0.0387  \n",
       "3    While there are no a priori rules about the p...  0.4588  \n",
       "4   Wow We reached K followers just within  weeks ...  0.9217  \n",
       "5   Central Banking created Debt Serfdom This is a... -0.1280  \n",
       "6   Professional Certifications that will help jum...  0.4019  \n",
       "7   I would not be interested in Bitcoin if govern...  0.1139  \n",
       "8   I will choose  Random Followers to Win  FOLLOW...  0.7925  \n",
       "9   Remiit a blockchainpowered remittance platform...  0.0000  \n",
       "10  The bear market came and it broke us Not in th... -0.8074  \n",
       "11  True story Sad and unfortunate that even great...  0.2023  \n",
       "12  Currently the transference of ownership or pur...  0.1779  \n",
       "13  How many days it took Bitcoin to get from    d...  0.0000  \n",
       "14  HODLBuckle inStop gawking at pricesGo to confe...  0.6908  \n",
       "15  Uzun zamandr ertelediim yeni yatrmc iin kripto...  0.0000  \n",
       "16  DeepVault is a blockchain notary that allows y...  0.0000  \n",
       "17  We are almost there MercaTox here we come PWRC...  0.0772  \n",
       "18  A silent night a star above a blessed gift of ...  0.9274  \n",
       "19   News Update Another pleasure to announce that...  0.8672  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Sentiment Analysis with Sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text  len\n",
       "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  115\n",
       "1          0  is upset that he can't update his Facebook by ...  111\n",
       "2          0  @Kenichan I dived many times for the ball. Man...   89\n",
       "3          0    my whole body feels itchy and like its on fire    47\n",
       "4          0  @nationwideclass no, it's not behaving at all....  111"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['sentiment','id','date','query_string','user','text']\n",
    "train = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", \n",
    "                    header=None, names=cols,\n",
    "                   encoding = \"latin1\")\n",
    "train = train[[\"sentiment\", \"text\"]]\n",
    "train[\"len\"] = [len(x) for x in train[\"text\"]]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    800000\n",
       "0    800000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing URL\n",
    "train[\"text\"] = [re.sub(\"http.{0,8}[A-Za-z0-9./]+\", '', x) for x in train[\"text\"]]\n",
    "\n",
    "#Removing @ tags\n",
    "train[\"text\"] = [re.sub(r'@[A-Za-z0-9]+','', x) for x in train[\"text\"]]\n",
    "\n",
    "#Removing HTML encoding\n",
    "train[\"text\"] = [re.sub(r'&[A-Za-z0-9]+','', x) for x in train[\"text\"]]\n",
    "\n",
    "#Removing Byte Order Mark\n",
    "train[\"text\"] = [re.sub(r'√Ø¬ø¬Ω', '?', x) for x in train[\"text\"]]\n",
    "\n",
    "#Removing Hashtags\n",
    "train[\"text\"] = [re.sub(r\"#\", ' ', x) for x in train[\"text\"]]\n",
    "\n",
    "#Removing Numbers\n",
    "train[\"text\"] = [re.sub(r\"[0-9]\", ' ', x) for x in train[\"text\"]]\n",
    "\n",
    "#Removing anything that's not a letter\n",
    "train[\"text\"] = [re.sub(r\"[^a-zA-Z]\", ' ', x) for x in train[\"text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla P40'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "sentences = [\"[CLS] \" + query + \" [SEP]\" for query in train[\"text\"]]\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = max([len(x) for x in tokenized_texts]) \n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(\"input_ids.pkl\", 'wb') as save_file:\n",
    "    pickle.dump(input_ids, save_file, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = pickle.load(open(\"input_ids.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train[\"sentiment\"].apply(lambda x: 1 if x == 4 else 0)\n",
    "\n",
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=42, test_size=0.2)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=42, test_size=0.2)\n",
    "                                             \n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs).long()\n",
    "validation_inputs = torch.tensor(validation_inputs).long()\n",
    "train_labels = torch.tensor(train_labels.values).long()\n",
    "validation_labels = torch.tensor(validation_labels.values).long()\n",
    "train_masks = torch.tensor(train_masks).long()\n",
    "validation_masks = torch.tensor(validation_masks).long()\n",
    "\n",
    "# Select a batch size for training. \n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader \n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n",
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.35164096132386474\n",
      "Train Accuracy: 0.84473046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [4:25:40<8:51:20, 15940.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.32798154949098823\n",
      "Validation Accuracy: 0.859209375 \n",
      "\n",
      "\n",
      "Train loss: 0.2966812129945494\n",
      "Train Accuracy: 0.8731515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [8:51:23<4:25:41, 15941.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.3314019749626517\n",
      "Validation Accuracy: 0.8602875 \n",
      "\n",
      "\n",
      "Train loss: 0.2506233406974003\n",
      "Train Accuracy: 0.8955125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [13:17:18<00:00, 15946.24s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.356741145048663\n",
      "Validation Accuracy: 0.85685 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=2e-5,\n",
    "                     warmup=.1)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis = 1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "  \n",
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "validation_loss_set = []\n",
    "\n",
    "train_accuracy_set = []\n",
    "validation_accuracy_set = []\n",
    "\n",
    "# Number of training epochs \n",
    "epochs = 3\n",
    "\n",
    "# BERT training loop\n",
    "for _ in trange(epochs, desc=\"Epoch\"):  \n",
    "  \n",
    "    ## TRAINING\n",
    "\n",
    "    # Set our model to training mode\n",
    "    model.train()  \n",
    "    \n",
    "    # Tracking variables\n",
    "    train_loss, train_accuracy = 0, 0\n",
    "    nb_train_steps, nb_train_examples = 0, 0\n",
    "    \n",
    "    # Tracking variables\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    # Train the data for one epoch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)        \n",
    "        lossout = loss(output, b_labels)   \n",
    "        # Backward pass\n",
    "        lossout.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "        # Update tracking variables\n",
    "        logits = output.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        tmp_train_accuracy = flat_accuracy(logits, label_ids)\n",
    "        train_accuracy += tmp_train_accuracy\n",
    "        nb_train_steps += 1\n",
    "        \n",
    "        tr_loss += lossout.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "    train_loss_set.append(tr_loss/nb_tr_steps)\n",
    "    train_accuracy_set.append(train_accuracy/nb_train_steps)\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    print(\"Train Accuracy: {}\".format(train_accuracy/nb_train_steps))\n",
    "\n",
    "       \n",
    "    ## VALIDATION\n",
    "\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_ev_steps, nb_eval_examples = 0, 0, 0\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "          # Forward pass, calculate logit predictions\n",
    "            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)    \n",
    "            lossout_val = loss(logits, b_labels)\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)    \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "        # Loss\n",
    "        eval_loss += lossout_val.item()\n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_ev_steps += 1\n",
    "    validation_loss_set.append(eval_loss/nb_ev_steps)\n",
    "    validation_accuracy_set.append(eval_accuracy/nb_eval_steps)        \n",
    "    print(\"Validation loss: {}\".format(eval_loss/nb_ev_steps))    \n",
    "    print(\"Validation Accuracy: {} \\n\\n\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"bert_state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running trained BERT on Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"final bitcoin.csv\")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "model.load_state_dict(torch.load(\"bert_state\"))\n",
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['[CLS]', 'the', 'first', 'lab', 'made', 'diamond', 'happened', 'in', 'amp', 'now', 'they', 'are', 'so', 'flawless', 'that', 'mined', 'diamonds', 'can', '##t', 'come', 'close', 'the', 'same', 'will', 'happen', 'to', 'gold', 'not', 'to', 'mention', 'the', 'massive', 'supply', 'available', 'on', 'asteroids', 'amp', 'other', 'sources', 'as', 'a', 'result', 'bit', '##co', '##in', 'is', 'a', 'necessary', 'part', 'of', 'human', 'evolution', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = df[\"text\"]\n",
    "\n",
    "# Tokenization\n",
    "sentences = [\"[CLS] \" + query + \" [SEP]\" for query in text]\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = max([len(x) for x in tokenized_texts]) \n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(input_ids).long().to(device)\n",
    "masks = torch.tensor(attention_masks).long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12565.857963562012\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start = time.time()\n",
    "\n",
    "logits = []\n",
    "for x in range(inputs.shape[0]):\n",
    "    with torch.no_grad():\n",
    "        logits.append(model(inputs[x].unsqueeze(0), token_type_ids=None, attention_mask=masks[0].unsqueeze(0))) \n",
    "    \n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ky848/.local/lib/python3.7/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "\n",
    "with open(\"logits.pkl\", 'wb') as save_file:\n",
    "    pickle.dump(logits, save_file, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = [x.cpu() for x in logits]\n",
    "final = [np.argmax(x[0]).item() for x in logits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sentiment140_sentiments.pkl\", 'wb') as save_file:\n",
    "    pickle.dump(final, save_file, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2b777aed48d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
